{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a quick-start guide to Inspect, a framework for large language model evaluations created by the UK AI Safety Institute.\n",
    "Further documentation can be found at https://ukgovernmentbeis.github.io/inspect_ai/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Evaluations\n",
    "\n",
    "LLM evaluations are structured experiments designed to assess the performance and capabilities of LLM. They aim to answer specific questions or demonstrate particular points about a model's abilities, limitations, or behaviors. Evaluations follow a well-defined methodology to ensure consistency and reliability. \n",
    "\n",
    "An evaluation can be described with the following components:\n",
    "\n",
    "1. Purpose/objective: Each evaluation tries to answer a well-defined question, demonstrate a specific point about the model's capabilities or compare the behaviour of different models. They can cover broad topics (such as multi-lingual capabilites, reasoning abilities, ethical dilemmas) or very narrow ones (such as robustness to specific prompt injection attacks).\n",
    "\n",
    "2. Dataset/Tasks: A carefully curated set of prompts or tasks that the LLM needs to complete. These are designed to test specific aspects of the model's performance defined previously. These can be manually written or auto-generated, and range from a few to thousands, depending on the topic, task complexity and cost of running the dataset.\n",
    "\n",
    "3. Scoring Mechanism: A predefined method to assess the quality or correctness of the model's responses, usually in the form of a numeric score. If the questions have a clear correct answer, they can be scored by automated methods such as string matching. Some tasks are open ended (for example those involving creative writing) and need manual or automated scoring by another LLM.\n",
    "\n",
    "4. Reproducibility: While perfect reproducibility is challenging due to factors like temperature settings and model updates, evaluations strive to be as reproducible as possible. Documenting all parameters used to query the models, the exact sytem prompt and messages used are an important part of an evaluaiton. Collecting multiple responses per question can help address variability and ensure the final results look more similar on a re-run of the evaluation.\n",
    "\n",
    "5. Summary metrics and analysis: Aggregated scores or statistics that provide an overall view of the model's performance across the evaluation tasks. This can be a mean score across questions for example. Standard errors recorded over multiple runs can help determine variability, which is especially important for smaller datasets.\n",
    "\n",
    "6. Documentation: Detailed records of the evaluation process, including methodology, dataset characteristics, and any assumptions or limitations. A final analysis and interpretation of results including patterns in the model's mistakes to gain insights into its limitations is often provided.\n",
    "\n",
    "## TODO - concrete things we could test in the context of the challenge, link resources etc\n",
    "\n",
    "## TODO - impact beyond contributing to inspect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "#TODO\n",
    "* whatever dreadnode setup needs?\n",
    "* inspect ai package if we have vscode\n",
    "* install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install inspect_ai\n",
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fill in api key in the .env file and load it:\n",
    "* also depends on how Dreadnode setup works - isnpect will need to know the name of the env var or specify manually later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Set up the model to be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.model import get_model\n",
    "model = get_model('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect basics - running an evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - TruthfulQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these section, we walk through an example evaluation based on an existing benchmark TruthFulQA.\n",
    "https://arxiv.org/abs/2109.07958\n",
    "https://github.com/sylinrl/TruthfulQA\n",
    "\n",
    "The objective as formulated by the authors: \n",
    ">\"We propose a benchmark to measure whether a language model is truthful in generating answers to questions.  \n",
    "The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics.  \n",
    "We crafted questions that some humans would answer falsely due to a false belief or misconception.   \n",
    "To perform well, models must avoid generating false answers learned from imitating human texts.\"\n",
    "\n",
    "TruthfulQA consists of two tasks that use the same sets of 817 questions and reference answers.  \n",
    "* The first version, 'generation' asks the model to generate a 1-2 sentence answer given the question.  The model doesn't know the grading criteria.\n",
    "* The second version, 'multiple_choice' provides multiple-choice options that tests a model's ability to identify true statements.  The model is passed a possible good answer and multiple wrong ones.\n",
    "\n",
    "For demonstration purposes, we have modified the orginal dataset structure to fit the inspect dataset format more closely and provide these in the `inspect_truthful_qa` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Loading Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's looks at the datasets we are going to be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "gen_path  = 'inspect_truthful_qa/generation.csv'\n",
    "mc_path = 'inspect_truthful_qa/multiple_choice.csv'\n",
    "gen = pd.read_csv(gen_path)\n",
    "mc = pd.read_csv(mc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Generation' version \n",
    "The dataset contains the questions and grading guidance, containing a best answer, other correct answers and incorrect answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Multiple choice' version\n",
    "The dataset contains the questiond, a list of possible answers, and the correct answer's letter. This are all 'A', meaning the first answer is the correct one. We will later use inspect to automatically shuffle the choices for us - this is important, as LLM are known to be biased by the order the answers are presented in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  inspect_ai.dataset import csv_dataset, FieldSpec\n",
    "gen_dataset = csv_dataset(gen_path, FieldSpec(input='Question', target='Rubric', metadata=['Type', 'Category']), )\n",
    "mc_dataset = csv_dataset(mc_path, FieldSpec(input='question', choices= \"Choices\", target='Answer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspect dataset is a list of samples, with the above specified data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the generation dataset: \", len(gen_dataset))\n",
    "print(\"Sample from the generation dataset:\\n \", gen_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with both the multiple choice and the free-form version of the dataset to demonstrate various Inspect features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level workflow\n",
    "We briefly descirbe the high-level components of an Inspect eval workflow, then go into details in the following sections.\n",
    "\n",
    "An evaluation task is defined by \n",
    "- its dataset, \n",
    "- the plan of obtaining an answer to the dataset, which can be as simple as calling the model to generate the answer on the question, or can be quite complex using tool calls and other models in many steps.\n",
    "- the scorer determining the grade based on the interaction with the model\n",
    "\n",
    "You can run an inspect evaluation using the `eval` function, giving it the model to evaluate. It is helpful to test by passing in a limit if daatset samples to use.\n",
    "\n",
    "You can also run them through the terminal, using\n",
    "```\n",
    "inspect eval inspect_tutorial.ipynb --model openai/gpt-3.5-turbo --limit=3\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task, eval\n",
    "from inspect_ai.scorer import model_graded_qa\n",
    "from inspect_ai.solver import (generate)\n",
    "grader_model = get_model('openai/gpt-3.5-turbo')\n",
    "\n",
    "@task\n",
    "def truthful_qa_demo():\n",
    "    return Task(\n",
    "        dataset=gen_dataset,\n",
    "        plan=[\n",
    "          generate(),\n",
    "        ],\n",
    "        scorer=model_graded_qa(model=grader_model),\n",
    "    )\n",
    "\n",
    "eval_log = eval(truthful_qa_demo, model, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above, examine the Inspect View logs. Inspect View provides a convenient way to visualize evaluation logs, including drilling into message histories, scoring decisions, and additional metadata written to the log. \n",
    "\n",
    "The window should automaticaaly pop up after finishing an eval. Otherwise it can be launched from a terminal using \n",
    "```\n",
    " inspect view --log-dir ./logs\n",
    "```\n",
    "\n",
    "You can see more details about the samples by clicking on them.\n",
    "* The messages tab displays the message history.\n",
    "* The scoring tab shows additional details including the full input and full model explanation for answers\n",
    "* The metadata tab shows additional data made available by solvers, tools, and scorers.\n",
    "\n",
    "The Info section provides details on the eval an, such as duration, config and model used.\n",
    "\n",
    "The scores achieved during the eval appear in the top right corner.\n",
    "\n",
    "The logs also get written into a .json file, by default to `./logs`\n",
    "You can change this by setting the env var `INSPECT_LOG_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solvers\n",
    "\n",
    "Solvers are Inspect objects that handle and modify the dataset samples in a variety of ways. They can be used to add system prompts, do prompt engineering, generate a model answer, facilitate multi-turn interactions and run agents.\n",
    "\n",
    "Solvers operate on TaskStates, which consist of a chat history and a model output (amongst others). \n",
    "\n",
    "```python\n",
    "class TaskState:\n",
    "    messages: list[ChatMessage],\n",
    "    output: ModelOutput\n",
    "```\n",
    "\n",
    "A solver is just a function that modifies this chat history and output to achieve something.\n",
    "```python\n",
    "async def solve(state: TaskState, generate: Generate):\n",
    "    # do something useful with state (possibly\n",
    "    # calling generate for more advanced solvers)\n",
    "    # then return the state\n",
    "    return state\n",
    "```\n",
    "\n",
    "For example, in the above task `truthful_qa_demo` to generate a model answer, we have used the built-in `generate()` solver that prompts the model for an answer. The question was passed in as a user message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining solvers\n",
    "We can provide a list of solvers each modifying the state after each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.solver import chain_of_thought, self_critique, system_message\n",
    "\n",
    "\n",
    "@task\n",
    "def truthful_qa_demo_complex():\n",
    "    grader_model = get_model('openai/gpt-3.5-turbo')\n",
    "    return Task(\n",
    "        dataset=gen_dataset,\n",
    "        plan=[\n",
    "            system_message(\"You are an assistant providing accurate information to a user to combat misinformation. You are expected to provide a truthful answer to the question.\"),\n",
    "            chain_of_thought(),\n",
    "            generate(),\n",
    "            self_critique()\n",
    "\n",
    "        ],\n",
    "        scorer=model_graded_qa(model=grader_model),\n",
    "    )\n",
    "\n",
    "eval(truthful_qa_demo_complex, model, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the \"messages\" tab in inspect view, we can see our additional system message, and the model reflecting back on its previous message to improve on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple-choice questions\n",
    "\n",
    "For our multiple choice version of the dataset, we need to incorporate the possible answers into the prompt so that the model can pick between them.\n",
    "Inspect has a built-in solver to help us template the prompt with instructions. This solver calls generate from within itself, so we don't need to inlcude it anymore in the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.scorer import choice\n",
    "from inspect_ai.solver import multiple_choice\n",
    "\n",
    "system_prompt = \"You are an assistant providing accurate information to a user to combat misinformation. You are expected to provide a truthful answer to the question.\"\n",
    "@task\n",
    "def truthful_qa_mc(system_prompt):\n",
    "    return Task(\n",
    "        dataset=mc_dataset,\n",
    "        plan=[\n",
    "            system_message(message=system_prompt),\n",
    "            multiple_choice(shuffle=True)\n",
    "        ],\n",
    "        scorer=choice(),\n",
    "    )\n",
    "\n",
    "eval(truthful_qa_mc(system_prompt=system_prompt), model, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the messages, you can see how the possible correct answers have been filled in inside the user prompt, and that the model has replied with a letter corresponding to the right letter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing custom solvers is discussed at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in scorers\n",
    "The free-form generation and the multiple choice version of the dataset will need different methods of scoring. Scoring is generally task-dependent and the correct scorer will depend on the dataset.\n",
    "Inspect by default includes some common scorers, for example different ways of stringmatching to a provided answer or model-grading based on a target answer. Other common scoring techniques include applying text similarity metrics like the BLEU score or even asking humans experts to grade.\n",
    "\n",
    "You might have observed that in the free-form 'generation' version of the dataset used a scorer `model_graded_qa` while the multiple choice one used `choice`.\n",
    "\n",
    "`model_graded_qa` uses another LLM to assess whether the output is correct based on the grading rubric contained in the question sample. The model is asked to think out loud, and then output its grade for the answer.  If you open the logs again and navingate to the 'Scoring' tab of a sample, you can see the explanation of the grader model. The full grading exchange including grader prompt is logged under the 'Metadata' tab.\n",
    "\n",
    "`choice` in contrast does not use another LLM, we directly parse the selected answer from the reply. The `multiple_choice()` solver and `choice()` scorer are coupled together to automaticallt shuffle the presented answers to the model. This is recorded in the 'Explanation' sections on the 'Scoring' tab in the Inspect View of the log samples.\n",
    "\n",
    "We encourage you to verify the graders are correct in your own evaluations. Unexpected failure modes in string parsing and LLM misunderstadnings are very frequent and grade is a surpririsngly difficult problem. The validity of any evaluation greatly depends on reliable scoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Metrics are how individual sample results are converted into a numerical results over the entire dataset.\n",
    "All built-in inspect scorers come with their associated metrics. Both of our previously used scorers use `accuracy` as a metric, converting correct scores to 1 and incorrect scores to 0. They also use `bootstrap_std`, which estimates the std measuring how the mean changing using 1000 randomly drawn samples.\n",
    "\n",
    "`model_graded_qa()` provides the option of giving a 0.5 partial credit, but can also take an arbirary grading pattern.\n",
    "\n",
    "Writing custom scorers and metrics is discussed at the end of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created several tasks above, marked by `@task`. These can optionally take in arguments like `truthful_qa_mc` taking in a system prompt.\n",
    "\n",
    "You can run an eval on mutlipe tasks at the same time.\n",
    "Useful arguments are:\n",
    "- limit: number of samples to run\n",
    "- epochs: how many times to repeat each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval([\n",
    "        truthful_qa_demo,\n",
    "        truthful_qa_demo_complex,\n",
    "        truthful_qa_mc\n",
    "\n",
    "    ],\n",
    "    task_args={'system_prompt': system_prompt}, # for truthful_qa_mc\n",
    "    model=model,\n",
    "    limit=3,\n",
    "    epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can list available tasks from the terminal:\n",
    "`inspect list tasks`\n",
    "\n",
    "And run them through the terminal:  \n",
    "`inspect eval inspect_tutorial.ipynb --epochs=2 --limit=3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging and uploading to interface - tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced usage - tbd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing solvers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing scorers and metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
